{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天模型-Groq\n",
    "\n",
    "[参考](https://python.langchain.com/v0.1/docs/integrations/chat/groq/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入ChatGroq类并用一个模型对它初始化\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# 可用的模型 https://console.groq.com/docs/models\n",
    "chat = ChatGroq(temperature=0, \n",
    "                model_name=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😊\n",
      "\n",
      "低延迟LLMs（Low-Latency Large Language Models）是指能够快速响应用户输入的语言模型。它们的重要性在于：\n",
      "\n",
      "1. **实时交互**：低延迟LLMs可以实时响应用户的输入，提供更加流畅的交互体验。例如，在聊天机器人或虚拟助手中，低延迟LLMs可以快速响应用户的询问和命令。\n",
      "2. **提高用户体验**：低延迟LLMs可以减少用户等待的时间，提高用户的体验感。例如，在搜索引擎中，低延迟LLMs可以快速返回搜索结果，提高用户的搜索体验。\n",
      "3. **支持实时应用**：低延迟LLMs可以支持实时应用，如语音识别、机器人等。例如，在语音识别中，低延迟LLMs可以快速识别语音，实时响应用户的命令。\n",
      "4. **提高模型的可靠性**：低延迟LLMs可以提高模型的可靠性。例如，在自动驾驶中，低延迟LLMs可以快速识别道路和交通状况，提高自动驾驶的可靠性。\n",
      "5. **支持 Edge Computing**：低延迟LLMs可以支持 Edge Computing，实时处理数据在 Edge 设备上，减少数据传输的延迟和提高实时应用的性能。\n",
      "\n",
      "综上所述，低延迟LLMs的重要性在于提高用户体验、支持实时应用、提高模型的可靠性和支持 Edge Computing。\n"
     ]
    }
   ],
   "source": [
    "# 编写一个提示并调用ChatGroq来创建完成\n",
    "\n",
    "# 系统提示\n",
    "system = \"你是一个有帮助的助手\"\n",
    "# 人员输入\n",
    "human = \"{text}\"\n",
    "# 提示模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",system),\n",
    "    (\"human\", human)\n",
    "    ])\n",
    "# 创建链\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = prompt | chat | StrOutputParser()\n",
    "# 调用链\n",
    "print(chain.invoke(\n",
    "    {\"text\": \"解释低延迟LLMs的重要性，请用汉语\"}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "九寨沟的太阳\n",
      "\n",
      "九寨沟，位于四川省阿坝藏族羌族自治州的西南部，是中国最美丽的景观之一。这里的自然美景，吸引了来自世界各地的游客。今天，我来到九寨沟，感受太阳的温暖和照耀。\n",
      "\n",
      "早上，我从酒店出发，步行到九寨沟的入口。太阳还没有升起，天空中是一片暗蓝色的。随着太阳的升起，天空逐渐变亮，云彩开始浮现。太阳升到半空，九寨沟的山峰和湖泊开始被照亮。山峰的颜色变得更加鲜艳，湖泊的水面开始泛起涟漾的波浪。\n",
      "\n",
      "我沿着九寨沟的步道，继续向前走。太阳的光芒照亮了我的脚步，照亮了九寨沟的每一个角落。山峰的树林开始被照亮，树叶的颜色变得更加鲜艳。湖泊的水面开始泛起涟漾的波浪，水中的鱼儿开始游动。\n",
      "\n",
      "太阳升到最高点，九寨沟的景色变得更加绮丽。山峰的颜色变得更加鲜艳，湖泊的水面变得更加平静。太阳的光芒照亮了九寨沟的每一个角落，照亮了我的心灵。\n",
      "\n",
      "我在九寨沟度过了一天，感受太阳的温暖和照耀。太阳的光芒照亮了九寨沟的每一个角落，照亮了我的心灵。九寨沟的太阳，真的太美丽了！\n"
     ]
    }
   ],
   "source": [
    "# Groq还支持异步和流功能\n",
    "# 初始化聊天模型\n",
    "chat = ChatGroq(temperature=0, \n",
    "                model_name=\"llama3-8b-8192\")\n",
    "# 定义提示模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\",\"写一篇九寨沟游记关于{topic}，请用汉语，不需要翻译\")\n",
    "    ])\n",
    "# 定义链\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = prompt | chat | StrOutputParser()\n",
    "# 异步调用链\n",
    "print(await chain.ainvoke({\"topic\":\"太阳\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "九寨沟的月亮\n",
      "\n",
      "我和朋友们来到九寨沟，期待着这个美丽的景观。我们来到这里的原因是，九寨沟的月亮是世界上最美丽的月亮之一。我们来到这里的目的就是为了见证这个美丽的月亮。\n",
      "\n",
      "当我们来到这里的时候，月亮已经升起了。我们来到这里的第一印象是，月亮的美丽程度远远超过了我们的想象。月亮的表面是那么的洁净，像是一块洁净的镜子。我们可以看到月亮的表面上有很多的山峰和沟壑，像是一幅美丽的画卷。\n",
      "\n",
      "我们来到这里的第二印象是，月亮的光芒是那么的柔和。月亮的光芒照亮了整个九寨沟，照亮了我们脚下的每一个地方。我们可以看到月亮的光芒照亮了整个景观，照亮了每一个角落。\n",
      "\n",
      "我们来到这里的第三印象是，月亮的美丽程度是那么的深刻。月亮的美丽程度是那么的深刻，我们可以看到月亮的美丽程度是那么的深刻，我们可以看到月亮的美丽程度是那么的深刻。\n",
      "\n",
      "总的来说，我们来到九寨沟的月亮是那么的美丽。我们来到这里的目的就是为了见证这个美丽的月亮，我们来到这里的目的就是为了见证这个美丽的月亮。"
     ]
    }
   ],
   "source": [
    "# 流式调用链\n",
    "# 定义链\n",
    "chain = prompt | chat\n",
    "for chunk in chain.stream({\"topic\":\"月亮\"}):\n",
    "    print(chunk.content,end=\"\",flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
