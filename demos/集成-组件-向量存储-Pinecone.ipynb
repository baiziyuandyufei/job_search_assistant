{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考\n",
    "\n",
    "- [https://python.langchain.com/v0.2/docs/integrations/tools/sql_database/](https://python.langchain.com/v0.2/docs/integrations/tools/sql_database/)\n",
    "- [https://python.langchain.com/v0.2/docs/integrations/providers/pinecone/](https://python.langchain.com/v0.2/docs/integrations/providers/pinecone/)\n",
    "- [https://python.langchain.com/v0.2/docs/integrations/vectorstores/pinecone/](https://python.langchain.com/v0.2/docs/integrations/vectorstores/pinecone/)\n",
    "- [https://api.python.langchain.com/en/latest/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html](https://api.python.langchain.com/en/latest/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html)\n",
    "- [https://app.pinecone.io/](https://app.pinecone.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feiyu/miniconda3/envs/langchain-py11/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/feiyu/miniconda3/envs/langchain-py11/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/Users/feiyu/miniconda3/envs/langchain-py11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_result[0]的维度: 1024\n",
      "doc_result[1]的维度: 1024\n",
      "doc_result[0]的模: 0.9999999668938295\n",
      "doc_result[1]的模: 1.0000000349748062\n",
      "doc_result[0]与doc_result[1]之间的相似度: 0.6709808574437868\n"
     ]
    }
   ],
   "source": [
    "# BAAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "model_name = \"BAAI/bge-large-zh-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "text_li = ['How is the weather today?', '今天天气怎么样?']\n",
    "doc_result = embedding.embed_documents(text_li)\n",
    "print(f\"doc_result[0]的维度: {len(doc_result[0])}\")\n",
    "print(f\"doc_result[1]的维度: {len(doc_result[1])}\")\n",
    "print(f\"doc_result[0]的模: {np.linalg.norm(doc_result[0])}\")\n",
    "print(f\"doc_result[1]的模: {np.linalg.norm(doc_result[1])}\")\n",
    "cosine_similarity_normalized = np.dot(doc_result[0], doc_result[1])\n",
    "print(f\"doc_result[0]与doc_result[1]之间的相似度: {cosine_similarity_normalized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "pc = Pinecone()\n",
    "index_name = \"horror-demo\"  # change if desired\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|████████████████████████████████████████████████| 197/197 [21:46<00:00,  6.63s/batch]\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Initialize the database\n",
    "db = SQLDatabase.from_uri(\"sqlite:///db/db/horror.db\")\n",
    "\n",
    "# Fetch the content\n",
    "result = db.run(\"SELECT content FROM wangyi LIMIT 100;\", fetch=\"cursor\")\n",
    "\n",
    "# Split content into multiple documents if the length exceeds 100 characters\n",
    "docs = []\n",
    "for doc in result.mappings():\n",
    "    content = doc['content']\n",
    "    while len(content) > 100:\n",
    "        split_content = content[:100]\n",
    "        docs.append(split_content)\n",
    "        content = content[100:]\n",
    "    if content:\n",
    "        docs.append(content)\n",
    "\n",
    "# Initialize the Pinecone vector store\n",
    "docsearch = PineconeVectorStore(embedding=embedding, index_name=index_name)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 10\n",
    "total_batches = len(docs) // batch_size + (1 if len(docs) % batch_size != 0 else 0)\n",
    "\n",
    "# Add documents in batches with a progress bar showing percentage\n",
    "with tqdm(total=total_batches, desc=\"Progress\", unit=\"batch\", ncols=100) as pbar:\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch_docs = docs[i:i + batch_size]\n",
    "        docsearch.add_texts(batch_docs)\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='车里伸出头嚷道：“前车加完没，还能动弹不，小伙儿，你见到鬼了，加完你倒是让他开走啊。”听到后车不友好的吵嚷，小涛非但不生气反倒高兴起来，心想：太好了，不是鬼车。这辆神秘的车也马上开走了。这一晚平安无事'\n",
      "page_content='伙人车控透过舷凑到一块，摆摆龙门阵谈论鬼神，也就当做消遣了。要说这里头最吸引人的，那该算是那些鬼啊神啊之类的段子了。谁谁谁喝喜酒喝醉了，走夜路碰到鬼，倒在坟地里，睡了一晚上。谁谁谁去河里洗衣服，看见河'\n",
      "page_content='5万简介：（灵车：运载灵柩或骨灰盒的车辆，你也可以理解为死人专用车。）我做了四年公交司机，心中的秘密也整整压抑了四年，我来亲身讲述你所不知道的列车惊悚事件。灵车改装成公交车之事，或许你没经历过，但你所'\n",
      "page_content='渔夫这才放下了悬着的心.......自此，这些所谓辟邪打招呼的方法也被广泛的应用起来，不光是因为怕撞到不干净的东西，更是让人们放下心理作用的一个方式。故事二：二中坡的酒后见鬼经历有一个在深圳湖贝路贩卖'\n"
     ]
    }
   ],
   "source": [
    "query = \"卡车司机的鬼故事\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Document 0\n",
      "\n",
      "呀\n",
      "\n",
      "## Document 1\n",
      "\n",
      "纸条上面写着：我们都很喜欢听你讲鬼故事……\n",
      "\n",
      "## Document 2\n",
      "\n",
      "、包容的姿态，欢迎全球各地的朋友前来观光旅游、投资兴业！\n",
      "\n",
      "## Document 3\n",
      "\n",
      "么随便的，没有休息好，就请假回去睡一觉。婷姐却不以为意，说伟哥平常都非常好说话，只要没什么活干，一般说身体不舒服，基本都批假，让我们休息时间，甚至外出，都不会管我们。“我刚刚见你没有去大保健，对你的印\n"
     ]
    }
   ],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"mmr\")\n",
    "query = \"卡车司机的鬼故事\"\n",
    "matched_docs = retriever.invoke(query)\n",
    "for i, d in enumerate(matched_docs):\n",
    "    print(f\"\\n## Document {i}\\n\")\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMR检索\n",
    "\n",
    "MMR (Maximal Marginal Relevance) 是一种用于提高检索结果多样性的算法。它的主要特点和工作原理如下:\n",
    "\n",
    "1. 目的:在保证结果相关性的同时,减少检索结果的冗余,提高多样性[1][2].\n",
    "\n",
    "2. 核心思想:通过贪心策略选择结果,在相关性和多样性之间寻求平衡[4].\n",
    "\n",
    "3. 算法流程:\n",
    "   - 首先选择与查询最相关的文档\n",
    "   - 然后迭代选择后续文档,每次选择时考虑两个因素:\n",
    "     a) 与查询的相关度\n",
    "     b) 与已选文档的差异度[2][4]\n",
    "\n",
    "4. 数学表达:\n",
    "   MMR = argmax[λ * Sim1(Di, Q) - (1-λ) * max Sim2(Di, Dj)]\n",
    "   其中:\n",
    "   - Sim1衡量文档与查询的相似度\n",
    "   - Sim2衡量文档间的相似度\n",
    "   - λ是平衡相关性和多样性的参数[4]\n",
    "\n",
    "5. 应用:广泛应用于搜索引擎、推荐系统等领域,用于结果重排序[4][5].\n",
    "\n",
    "6. 优点:\n",
    "   - 实现简单,计算效率高\n",
    "   - 可以有效提高结果多样性\n",
    "   - 参数λ可调,灵活控制相关性与多样性的权重[2][4]\n",
    "\n",
    "7. 扩展:可以根据具体应用场景,选择不同的相似度计算方法,如基于内容特征、用户行为等[4].\n",
    "\n",
    "MMR算法通过在相关性和多样性之间寻求平衡,能够有效提升检索结果的质量,为用户提供更全面、丰富的信息。\n",
    "\n",
    "Citations:\n",
    "- [1] https://ask.zkbhj.com\n",
    "- [2] https://blog.csdn.net/qq_39388410/article/details/109706683\n",
    "- [3] https://blog.csdn.net/sjyttkl/article/details/106896936\n",
    "- [4] https://juejin.cn/post/7292628510844223523\n",
    "- [5] https://www.youtube.com/watch?v=tCa4yackga0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 车里伸出头嚷道：“前车加完没，还能动弹不，小伙儿，你见到鬼了，加完你倒是让他开走啊。”听到后车不友好的吵嚷，小涛非但不生气反倒高兴起来，心想：太好了，不是鬼车。这辆神秘的车也马上开走了。这一晚平安无事 \n",
      "\n",
      "2. 5万简介：（灵车：运载灵柩或骨灰盒的车辆，你也可以理解为死人专用车。）我做了四年公交司机，心中的秘密也整整压抑了四年，我来亲身讲述你所不知道的列车惊悚事件。灵车改装成公交车之事，或许你没经历过，但你所 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"卡车司机的鬼故事\"\n",
    "found_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)\n",
    "for i, doc in enumerate(found_docs):\n",
    "    print(f\"{i + 1}.\", doc.page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `k` 和 `fetch_k` 两个参数的含义:\n",
    "\n",
    "在 `max_marginal_relevance_search` 函数中,`k` 和 `fetch_k` 这两个参数在最大边际相关性(MMR)搜索算法中扮演不同的角色:\n",
    "\n",
    "1. `k`: 这个参数决定了最终返回的文档数量。它代表了MMR算法最终输出的多样化且相关的结果数量。\n",
    "\n",
    "2. `fetch_k`: 这个参数指定了在应用MMR算法之前初始检索的文档数量。它决定了从向量存储中最初获取多少文档来考虑多样性和相关性。\n",
    "\n",
    "MMR算法的工作流程如下:\n",
    "\n",
    "1. 首先检索与查询最相似的 `fetch_k` 个文档。\n",
    "2. 然后从这 `fetch_k` 个文档中,使用MMR标准选择 `k` 个文档,这个标准在查询相关性和所选文档的多样性之间取得平衡。\n",
    "\n",
    "通过将 `fetch_k` 设置得比 `k` 大,你允许算法考虑更大范围的潜在相关文档,这可能会导致最终结果更加多样化。当你想增加返回文档的多样性,同时又要保持与查询的相关性时,这种方法特别有用。\n",
    "\n",
    "例如,如果你设置 `k=2` 和 `fetch_k=10`,该函数将:\n",
    "\n",
    "1. 最初检索与查询最相似的10个文档。\n",
    "2. 从这10个文档中,根据MMR标准选择并返回最能平衡相关性和多样性的2个文档。\n",
    "\n",
    "这种方法允许在结果多样性和计算成本之间进行权衡,因为增加 `fetch_k` 会提供更多的多样化选择选项,但也需要更多的处理时间。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
