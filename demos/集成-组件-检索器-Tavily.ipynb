{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[本笔记参考](https://python.langchain.com/v0.1/docs/integrations/retrievers/tavily/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然语言处理（NLP）相关的职位有：\n",
      "\n",
      "1. NLP Engineer/Scientist: 需要具备自然语言处理、机器学习、深度学习等相关技能，并熟悉常用的NLP工具和框架，如 NLTK、Spacy、Stanford NLP、TensorFlow、PyTorch等。\n",
      "\n",
      "2. Language Model Engineer: 需要掌握语言模型的构建和优化技能，如Transformer、LSTM、RNN等。\n",
      "\n",
      "3. Conversational AI Engineer: 需要掌握对话系统的设计和实现，如聊天机器人、语音助手等。\n",
      "\n",
      "4. Text Analysis Engineer: 需要掌握文本分析技能，如情感分析、实体识别、摘要生成等。\n",
      "\n",
      "\n",
      "自然语言处理与计算语言学的关系是：自然语言处理是计算语言学的一个应用分支，它利用计算机技术和机器学习方法来处理和分析自然语言，实现自动化的语言理解和生成。\n",
      "计算语言学则是研究自然语言的结构和规律，以及如何利用计算机技术来模拟和 simulate human language processing。\n",
      "\n",
      "\n",
      "自然语言处理的普遍要求包括：\n",
      "\n",
      "1. 扎实的计算机基础知识，包括数据结构、算法、操作系统、网络等。\n",
      "\n",
      "2. 熟悉常用的编程语言，如Python、Java、C++等。\n",
      "\n",
      "3. 理解自然语言处理的基本概念和技术，如词向量、语言模型、序列标注、依存分析、情感分析等。\n",
      "\n",
      "4. 熟悉机器学习和深度学习的基本原理和算法，如线性回归、支持向量机、卷积神经网络、递归神经网络等。\n",
      "\n",
      "5. 熟悉自然语言处理的工具和框架，如 NLTK、Spacy、Stanford NLP、TensorFlow、PyTorch等。\n",
      "\n",
      "6. 良好的英文阅读和沟通能力，因为大部分的NLP研究和应用文献都是英文的。\n",
      "\n",
      "7. 对自然语言和语言学有基本的了解和兴趣，包括语法、语义、语音、信息结构等方面的知识。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# 检索器\n",
    "retriever = TavilySearchAPIRetriever(k=3)\n",
    "# 语言模型\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\",temperature=0)\n",
    "# 提示\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"仅基于提供的上下文用汉语回答问题\n",
    "上下文: {context}\n",
    "\n",
    "问题: {question}\"\"\"\n",
    ")\n",
    "# 集成检索器到链中\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: x[\"question\"]) | retriever) # 传递question和context\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "result = chain.invoke({\"question\": \"自然语言处理相关职位有哪些？请列出相关职位的普遍要求。自然语言处理与计算语言学的关系是什么？\"})\n",
    "print('。\\n'.join(result.split('。')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral-8*7b知识补充\n",
    "\n",
    "### 1. Mixtral-8x7B-Instruct是Mistral AI公司发布的一个指令微调版大语言模型,具有以下主要特点:\n",
    "\n",
    "1. 基于Mixtral-8x7B基础模型进行指令微调。Mixtral-8x7B是一个稀疏混合专家(Sparse Mixture-of-Experts, SMoE)模型,总参数量约为47B[1][3].\n",
    "\n",
    "2. 采用监督微调和直接偏好优化算法(Direct Preference Optimization, DPO)进行优化,以更好地遵循人类指令[3].\n",
    "\n",
    "3. 支持多种语言,包括英语、法语、德语、西班牙语和意大利语[1].\n",
    "\n",
    "4. **最大支持32K tokens的上下文长度[1].**\n",
    "\n",
    "5. 在多项基准测试中表现优异,超越了许多其他开源模型的指令微调版本[3].\n",
    "\n",
    "6. 使用Apache 2.0许可证发布,允许商业使用[1].\n",
    "\n",
    "7. 具有简单的对话格式,便于使用[1].\n",
    "\n",
    "8. 在数学、代码生成和多语言任务上表现尤为出色[3].\n",
    "\n",
    "**尽管名为\"8x7B\",但Mixtral-8x7B-Instruct的实际参数量约为45B。对于每个token,模型会从8个专家网络中选择2个进行处理,因此每个token实际使用的活跃参数约为13B,这使得其推理速度与13B参数规模的模型相当[1][3].**\n",
    "\n",
    "总的来说,Mixtral-8x7B-Instruct是一个强大的开源指令微调模型,在多个领域展现出优秀的性能,为开发者和研究人员提供了一个有价值的工具。\n",
    "\n",
    "Citations:\n",
    "\n",
    "- [1] https://www.cnblogs.com/huggingface/p/17944988\n",
    "- [2] https://blog.infuseai.io/8x7b-moe-introduction-8295c612aa31?gi=b5c85e09aa0e\n",
    "- [3] https://help.aliyun.com/zh/pai/user-guide/finetune-and-deploy-mixtral-8x7b-moe-model\n",
    "- [4] https://juejin.cn/post/7322664566718414900\n",
    "- [5] https://www.163.com/dy/article/IOJ8G9JH0511AQHO.html\n",
    "\n",
    "### 2. Mixtral 8x7B模型支持的32K tokens上下文长度确实包括了输入和输出的总和。具体来说:\n",
    "\n",
    "1. 32K tokens指的是模型可以处理的总token数量,包括输入的提示(prompt)和模型生成的输出[1][3].\n",
    "\n",
    "2. 这个长上下文能力使得Mixtral 8x7B能够处理非常长的文本,例如进行书籍摘要、支持长期对话,以及处理涉及复杂推理步骤的任务[2].\n",
    "\n",
    "3. 在实际使用中,用户可以在32K tokens的限制内灵活分配输入和输出的长度。例如,可以输入一个很长的文档,然后要求模型生成简短的摘要;或者进行多轮对话,累积的对话历史和新的回复总长度不超过32K tokens[1][3].\n",
    "\n",
    "4. 这种长上下文能力是通过特殊的预训练方法实现的。Mixtral 8x7B在预训练过程中使用了不同长度的数据,包括长度达32K tokens的语料[2].\n",
    "\n",
    "5. 值得注意的是,虽然模型支持32K tokens的上下文,但在实际应用中,处理如此长的序列可能会带来较高的计算成本和延迟[3].\n",
    "\n",
    "总之,Mixtral 8x7B的32K tokens上下文长度为输入和输出提供了很大的灵活性,使其能够处理各种需要长期记忆或复杂推理的任务。\n",
    "\n",
    "Citations:\n",
    "\n",
    "- [1] https://blog.csdn.net/v_JULY_v/article/details/135176583\n",
    "- [2] https://juejin.cn/post/7355798869110554643\n",
    "- [3] https://www.jiqizhixin.com/articles/2024-01-10-6\n",
    "- [4] https://m.163.com/dy/article/J4M4N8RF0511DPVD.html\n",
    "- [5] https://www.cnblogs.com/huggingface/p/17944988"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
