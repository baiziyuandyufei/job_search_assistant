{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runable Sequence 链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why are wolves always asking for directions?\\n\\nBecause they have \"fleas\" for navigation! (sounds like \"GPS\")\\n\\nI hope you found this joke about a wolf\\'s sense of direction amusing. If you need more laughter or have any other requests, feel free to ask!', response_metadata={'token_usage': {'prompt_tokens': 20, 'total_tokens': 86, 'completion_tokens': 66}, 'model_name': 'accounts/fireworks/models/mixtral-8x7b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-26013abc-859a-4f28-aca3-24d825fe3daf-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runable Sequence 链\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Prompt接收字典对象，返回PromptValue对象\n",
    "prompt = ChatPromptTemplate.from_template(\"讲一个关于{topic}的笑话。\")\n",
    "# ChatModel接收PromptValue对象，返回ChatMessage对象\n",
    "chat = ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")\n",
    "# 注意Runnable Sequence 链中各结点的输入和输出\n",
    "chain = prompt|chat\n",
    "# 注意Runnabel Sequence 链的输入和输出\n",
    "chain.invoke({\"topic\":\"狼\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': AIMessage(content='小白兔为什么总是开心？因为它知道它不会被猎杀，因为它是“兔”斯基！(Rabbit-Ski)\\n :)\\n\\n(请注意，这是一句 humor/梗话，不是事实。)', response_metadata={'token_usage': {'prompt_tokens': 22, 'total_tokens': 97, 'completion_tokens': 75}, 'model_name': 'accounts/fireworks/models/mixtral-8x7b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-2ca97496-53a8-4cbb-ad12-9ab942336362-0'),\n",
       " 'poem': AIMessage(content=\"小白兔在草地上，\\nPlaying in the green, green grass,\\nSoft and fluffy, full of fun,\\nHopping here and there, so fast.\\n\\nWith twinkling eyes and pink little nose,\\nNibbling on clover, carrots too,\\nIn the sunshine, basking,\\nLiving life, simple and true.\\n\\nDreaming under the moon's soft glow,\\nIn the world of nature, free to go,\\nThe little bunny, full of woe,\\nA symbol of peace, we should know.\", response_metadata={'token_usage': {'prompt_tokens': 24, 'total_tokens': 157, 'completion_tokens': 133}, 'model_name': 'accounts/fireworks/models/mixtral-8x7b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-ca01a33e-ac9c-456b-8765-47115035b4ab-0')}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "import os\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# 使用Mistral-7b\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")\n",
    "# 定义笑话链\n",
    "joke_chain = ChatPromptTemplate.from_template(\"讲一句关于{topic}的笑话\")|llm\n",
    "# 定义诗歌链\n",
    "poem_chain = ChatPromptTemplate.from_template(\"写一首关于{topic}的短诗\")|llm\n",
    "# 通过RunnableParallel执行两个调用链\n",
    "map_chain = RunnableParallel(joke=joke_chain,poem=poem_chain)\n",
    "map_chain.invoke({\"topic\":\"小白兔\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The sum of 3 and 9 is 12. Is there anything else you would like to know? I'm here to help with any questions you have. Just let me know!\", response_metadata={'token_usage': {'prompt_tokens': 14, 'total_tokens': 55, 'completion_tokens': 41}, 'model_name': 'accounts/fireworks/models/mixtral-8x7b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-faae7829-ee8e-4ed7-850b-d82920093ea8-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_fireworks import ChatFireworks\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "def multiple_length_function(_dict):\n",
    "    return len(_dict[\"text1\"]) * len(_dict[\"text2\"])\n",
    "prompt = ChatPromptTemplate.from_template(\"What is {a}+{b}\")\n",
    "model = ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")\n",
    "chain = {\"a\": itemgetter(\"foo\")|RunnableLambda(length_function),\n",
    "         \"b\": {\"text1\":itemgetter(\"foo\"), \"text2\":itemgetter(\"bar\")} | RunnableLambda(multiple_length_function)}|prompt|model\n",
    "chain.invoke({\"foo\":\"bar\",\"bar\":\"gah\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_classify_chain 类型:  <class 'langchain_core.runnables.base.RunnableLambda'>\n",
      "question_retrieval_chain 类型:  <class 'langchain_core.runnables.base.RunnableLambda'>\n",
      "parallel_chain 类型:  <class 'langchain_core.runnables.base.RunnableParallel'>\n",
      "prompt 类型:  <class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='合并两个链的结果classify response和retrieval response。')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设计求职助手链\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def label_to_response(label):\n",
    "    return \"classify response\"\n",
    "\n",
    "def question_retrieval(text):\n",
    "    return \"retrieval response\"\n",
    "\n",
    "# 定义RunnableLambda对象，接收一个字符串，返回字符串\n",
    "question_classify_chain = RunnableLambda(label_to_response)\n",
    "print(\"question_classify_chain 类型: \", type(question_classify_chain))\n",
    "# 定义RunnableLambda对象，接收一个字符串，返回字符串\n",
    "question_retrieval_chain = RunnableLambda(question_retrieval)\n",
    "print(\"question_retrieval_chain 类型: \", type(question_retrieval_chain))\n",
    "\n",
    "# 定义RunnableParallel对象，接收多个Runnable对象，返回词典\n",
    "parallel_chain = RunnableParallel(question_classify_response=question_classify_chain,\n",
    "                                  question_retrieval_response=question_retrieval)\n",
    "print(\"parallel_chain 类型: \", type(parallel_chain))\n",
    "# 定义ChatPrompt对象，接收词典，返回PromptValue对象\n",
    "prompt = ChatPromptTemplate.from_template(\"合并两个链的结果{question_classify_response}和{question_retrieval_response}。\")\n",
    "print(\"prompt 类型: \", type(prompt))\n",
    "# 总链为并行链 | 系统提示\n",
    "final_chain = parallel_chain | prompt\n",
    "final_chain.invoke(\"你好！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
