我在自然语言处理领域积累了丰富的实践知识。我不仅熟悉传统算法如MaxEnt、LSTM和CRF，还对大语言模型有深入的理解和应用能力，特别是熟练掌握Transformers库、LangChain库。我在文本分类任务上成功微调过DistilBERT模型，并在实体识别任务上微调过Mistral7B模型。除此之外，我喜欢探索新技术，并在实践中积极使用fastGPT和LangChain搭建个人本地问答系统。我具备良好的团队协作精神和沟通技巧，能够快速融入新的工作环境并推动项目的顺利进行。
工作经历与精选项目
对话系统设计 2024年5月至今
大模型算法/LangChain、Prompt、RAG、Summarize、QA
•	基于LangChain的求职助手 项目链接 https://baiziyuandyufei-langchain-self-stu-my-streamlit-example1-laktvh.streamlit.app/ 
该项目旨在开发一个基于 Langchain和 llama-v3-70b-instruct  模型的对话机器人，主要用于帮助用户回答 HR  提出的问题或陈述。项目涉及多项前沿技术，特别是针对简历 PDF  文档和 JD  描述网页等外部知识的有效利用。
1. HR问题分类链：
此步的目的是识别出特定问题，针对特定问题给出特定的提示，进而给出特定的回答。
少样本提示模板如下：
给出每个文本的类别，类别只能属于以下列出的一种

- 离职原因
- 薪资
- 外包&外协&外派&驻场
- 兼职
- 学历

如果不属于以上类别，则类别名称为“其他”。

例如：

文本: 离职/换工作的原因
类别: 离职原因

文本: 你好，我们是外协岗位，在国家电网 南瑞工作的
类别: 外包&外协&外派&驻场

文本: 但是我们应该最高30K，一般还达不到.
类别: 薪资

文本: 哈喽～本职位为线上兼职，一单一结款，根据自己时间自由接单，不耽误自己的主业，您看感兴趣嘛？
类别: 兼职

文本: 你好
类别:
2. 个人简历知识：
针对简历 PDF  文档和 JD  描述网页等外部资源，使用文档加载器和文档转换器将其内容提取并结构化处理。使用 FAISS  向量存储器和检索器，将外部知识融入到对话中，提供更加精准和上下文相关的回答。
- 简历文档加载。使用 UnstructuredWordDocumentLoader 加载Word文档。
- 分档拆分。使用 `RecursiveCharacterTextSplitter` 拆分文档。关键参数解释如下：separators：定义多个分隔符。尽量使用多种分隔符有助于更细粒度地拆分文档，保证文本块的语义完整性和逻辑连贯性。chunk_size：设置为50字符，确保每个文本块适中，利于后续处理和检索效率。chunk_overlap：设置为20字符，使文本块之间有部分重叠，有助于提高检索内容的相关性和语境理解。
- 块向量化。向量化模型的选择与评测方法。在文本向量化中，使用paraphrase-multilingual-MiniLM-L12-v2对文本进行向量化，并评估模型的准确性。构建相关度数据集的方法包括通过增删改一定比例的字符生成相关句子，同时随机生成不相关句子。针对每个实例组，计算向量化后的文本之间的相似度，并根据预先指定的阈值进行准确性评估，最终得出平均准确性，这有助于提高检索时的相关性。
- 向量存储与检索。我使用了FAISS工具库，通过将文档集合转换为索引的方式实现了高效的文本检索。在检索过程中，我输入待检索的文本，并利用构建的索引进行相似度搜索，最终获取了与输入文本最相关的文档及其相关性分数，并按相关性排序输出。此外，我还对检索结果进行了重排序，利用最长公共子串算法提高了结果的质量和相关性。
3. 多轮交互与对话记忆：
实现对话记忆功能，确保多轮对话中上下文信息的保留和连续性。
4. 基于LCEL融合分类链与记忆链：
在我的项目中，我设计并实现了一种智能的求职助手，它能够根据用户提出的问题类型使用不同的提示，从而使LLM模型（大语言模型）生成更具个性化的回答。为了实现这一点，我引入了RunnableLambda链，具体过程如下：
- 首先，我定义了一系列模板，包括系统消息模板（SystemMessagePromptTemplate）和人类消息模板（HumanMessagePromptTemplate），这些模板以汉语进行交流，确保整个对话过程符合目标语言的要求。接着，我通过ChatPromptTemplate将这些消息模板组合成一个统一的聊天提示。
- 然后，我使用了一个名为RunnableLambda的可运行函数，通过它来实现问题分类功能。具体而言，我创建了一个数据处理链，其中RunnableLambda根据输入问题调用self.question_classify方法对问题进行分类。分类后的问题和原始问题一起被传递到下一个处理节点。根据不同的问题类别，该链条动态选择相应的提示，从而使得LLM模型可以生成更准确和相关的回答。
通过这种方法，我实现了一个智能的求职助手，它不仅能理解用户的多样化问题，还能根据问题的具体类型提供量身定制的回答。这种解决方案显著提高了用户互动的体验和回答的准确性。。
5. API  服务与交互界面设计：
在本项目中，我构建了一个个人求职助手聊天机器人，旨在帮助用户模拟与HR的对话，提供专业的求职建议和回答。该应用基于Streamlit框架，实现了与大型语言模型llama-v3-70b-instruct的交互。以下是本项目的API服务与交互界面设计说明。
- 技术栈。Streamlit：用于快速构建交互式Web应用。LangChain Fireworks：用于调用和管理语言模型。Dotenv：用于加载环境变量，确保敏感信息安全。Python：编程语言。
- API服务设计。环境变量配置：通过load_dotenv()函数加载环境变量，确保模型API密钥等敏感信息不被硬编码。语言模型初始化：使用ChatFireworks初始化llama-v3-70b-instruct模型，设置温度和top_p参数，保证生成响应的多样性和质量。提示模板配置：利用SystemMessagePromptTemplate和HumanMessagePromptTemplate定义系统和用户提示模板，使得模型能根据不同角色生成相应的回复。
- 交互界面设计。页面布局：标题部分：通过st.title()设置应用的主标题和副标题，直观展示应用功能。页面描述：使用st.caption()简要描述应用的功能和特点。侧边栏：提供密码输入框、API申请链接、源码链接以及在GitHub Codespaces中打开的快捷链接。会话管理：初始化会话：在st.session_state中存储会话消息，初次加载时添加助手的欢迎消息。显示消息：通过遍历st.session_state.messages，将所有会话消息展示在界面上。用户输入与响应生成：聊天输入表格：使用st.chat_input()获取用户输入，并通过海象运算符进行赋值和检查。消息处理：将用户输入添加到会话消息中，调用对话链chain.invoke({'input':prompt})获取模型生成的响应，并将响应添加到会话消息中。
- 功能特点。实时对话：用户可以实时输入HR的问题，模型即时生成并返回相应的回复。会话记录：所有对话记录保存在会话状态中，用户可以查看整个对话的上下文。安全性：通过环境变量和密码输入框，确保API密钥等敏感信息的安全性。
中科院信工所（非外协）2018年8月至2024年5月 
自然语言处理工程师/短文本分类、短文本实体识别
•	工作内容
我独自负责了两个关键的自然语言处理项目。首先是社交短文本分类项目，我完成了数据获取、语料清洗、模型构建和部署等任务。其次是开发高效的社交短文本实体识别系统，我利用Mistral 7B大语言模型进行实体信息提取，完成了整个项目的开发和实施。
•	社交短文本实体识别项目
本项目旨在开发一种高效的社交短文本实体识别系统，通过使用Mistral 7B大语言模型，从社交短文本中准确提取日期、设施、人物、货币、组织、地点、产品和事件等实体信息。我利用Mistral 7B模型的强大语言理解和生成能力，并进行4位量化以降低计算资源需求，添加LoRA适配器以提升特定任务表现，并通过自定义回调函数实时监控训练效果。我在微调Mistral 7B大语言模型方面积累了丰富的项目经验，确保其在不同任务上的高效表现。该系统主要为其他项目组提供自然语言处理基础组件。
1. 数据准备。
训练集句子总量: 200
验证集句子总量: 20
测试集句子总量: 200
类别总数: 19
2. 提示模板
根据实体标注任务，设计特征模板。
Extract the entities for the following labels from the given text and provide the results in JSON format
- Extract entities exactly as mentioned in the text.
- Return each entity under its label without creating new labels.
- Provide a list of entities for each label; if no entities are found, return an empty list.
- Accuracy and relevance are key.
Labels:
- ORG:Media
- GPE:Population-Center
- PER:Individual
- GPE:Nation
- ORG:Sports
...
3. 机器环境
Docker 环境配置，从github的Transformers库下载Dockerfile，build镜像。
Dirver Version: 535.104.05
CUDA Version: 12.2
GPU Name: Tesla T4
GPU Memory: 15360 MiB
4. 基本原理
- 使用BNB对Mistral-7b模型量化后加载到GPU。
- 利用PEFT技术增加LoRA层，有监督微调模型参数。
- 最大化next token概率。
5. 参数配置
- 周期数：3。
- 批大小：4。
- 累积梯度更新步数：2。
6. 训练过程
[75/75 1:39:05, Epoch 3/3]
Step	Training Loss	Validation Loss
10	1.061800	16.022060
20	0.386200	19.186018
30	0.268000	20.003162
40	0.255600	20.269028
50	0.240800	20.377626
60	0.225100	20.743212
70	0.214700	20.486006
7. 推断
加载量化基础模型Mistral-7b，设置生成任务相关参数，输入待标注句子，得到Json格式标注结果。
max_new_tokens=200,
early_stopping=True,
pad_token_id=tokenizer.eos_token_id,
eos_token_id=tokenizer.convert_tokens_to_ids("}"),
temperature=0.8,
top_k=50,
top_p=0.9,
repetition_penalty=1.2
8. 评测
去年“六一”前夕，总书记走进北京育英学校学生农场，看到孩子们正在开展农业种植活动。总书记说：“很多知识和道理都来自劳动、来自生活。引导孩子们从小树立劳动观念，培养劳动习惯，提高劳动能力，有利于他们更好地学习知识。”既有言传，又有身教，勤劳的意义愈发凸显.
{
 "PER:Individual": [
   "总书记"
 ],
 "ORG:Educational": [
   "北京育英学校",
   "北京育英学校学生农场"
 ]
}
•	社交短文本分类项目
1. 数据的获取
- 爬取 sougou  词库词典，解码转为普通可读文本文件。作用为为后续基于词典预测文本类别。制作链接爬虫，记录链接地址和链接名称，构建链接库。作用为根据用户提供的类别名称快速检索到对应的文本语料。制作语料爬虫。 提供两种爬虫形式。形式一：基于 scrapy  爬取链接库中的所有链接对应的语料，实现定期更新语料库。形式二：单链接爬虫。只爬取一个链接对应的全部语料。
- 交互界面设计与制作。基于flask+jquery+bootstrap+ajax  制作链接库管理界面和爬虫界面。功能包括：增删改查链接，爬取按钮，爬取过程日志的显示，类目体系显示，类目数据量分布显示，类目下文本显示。
2. 基于朴素贝叶斯模型净化训练语料
- 商品标题领域和新闻标题领域选择贝努力还是多项式分类模型？本系统使用多项式。
汉语分词与训练速度的关系。并没有使用去停用词或保留关键词，因为社交文本用词丰富，没有词表可囊括。只用词长和词形去掉没有意义的词语。
- 进一步优化了训练数据，根据混淆矩阵分布均匀的列去除了一个类目，分布均匀的行去除了一个类目，以确保数据的平衡性。同时，我根据训练好的模型，找出了每个类别的关键词，作为该类别的代表特征，用于净化训练语料。此外，我也过滤了数据量小于400的类目，以提高模型的泛化能力和分类效果。这些优化措施有助于提高模型对各类别的识别准确度和分类性能。
- 获取各类别关键词。基于训练好的模型，找出每个类别的关键词。模型训练好后，找出各类别得分最高的词语，就是该类别的关键词，传统模型的优势就在这里。使用类别关键词净化训练语料。
3. 基于 distilbert base multilingual cased  模型构建文本分类模型
- 为什么要用 distilbert ？使用 DistilmBERT  模型的原因是它在与朴素贝叶斯模型相比具有更好的性能和效率。首先，DistilmBERT  是一个经过蒸馏的模型，相比于朴素贝叶斯模型，它能够捕捉更复杂的语言结构和语义信息，从而提高了分类任务的准确性。其次，DistilmBERT  在处理多语言文本时也表现出色，因为它是在包含104种不同语言的维基百科数据上训练的，具有更好的泛化能力。与 bert base chinese  相比，DistilmBERT  的优势在于其更小的模型尺寸和更高的运行速度。虽然 BERT base chinese  是专门为处理中文文本而设计的，但它的模型参数更多，运行速度较慢。相比之下，DistilmBERT  在保持相近性能的情况下，模型更为轻量化，速度更快，适用于需要快速推理的场景。
使用新闻内容长文本微调大模型。
- 微调模型的步骤及注意点。（1）分词以及编码：此部分主要完成分词，填充，截断，掩码的生成。注意 cpu  内存受限时，需要对训练集分批次处理。（2）实现自定义 Dataset类，transformer  训练类只接受该类型的数据。重写初始化方法，获取元素方法，统计长度方法。使用编码后的数据集初始化该类的实例，后续实例化训练类。（3）定义模型评价方法。计算模型准确率和召回率。（4）设置训练超参数。周期数应该设置多少？三周期内不收敛说明代码中存在问题。批次大小主要取决于 GPU  内存。较大的批次，模型收敛快，但可能会陷入局部最优解，出现过拟合。较小的批次，提高模型泛化能力，可以让模型学习到不同样本之间的差异。学习率设置应当适中。过大，损失值不断增加，模型不收敛。过小，收敛速度较慢。（5）保持模型和分词编码器。如果想保存完整模型，需要将模型和分词器同时保存起来，便于以后加载自己微调后的模型，而无需再加载预训练的语言模型。
4. 模型预测。
支持单条和批量预测，满足用户要求。
长文本的预测。语种识别，汉英分词与关键词抽取。受模型对输入文本长度的限制，也是为了确保模型预测准确性，对于所有输入文本，加入预处理步骤，执行语种识别，分词和关键词抽取，将抽取出的关键词送入模型进行预测。
5. 上线部署。
6. Fastapi+uvicorn  封装为 docker  镜像，locust  负载测试。I7cpu 32g ram ，每秒请求数33，平均响应时间275ms。
7. 评测
- GPU配置：Tesla T4 GPU，15G内存。
- 数据比例：Number of Classes: 31 Train Size: 54660 Val Size: 13665 Test Size: 17082
- 数据比例净化后：Number of Classes: 16 Train Size: 18684 Val Size: 4672 Test Size: 5839
- 微调日志净化后： [3504/3504 51:08, Epoch 3/3]
Step Training Loss Validation Loss Accuracy F1
500	1.135300	0.313274	0.953125	0.952321
3500	0.078400	0.107589	0.977740	0.977640
- 分类报告：
accuracy 0.74 17082
macro avg 0.34 0.39 0.36 17082
weighted avg 0.65 0.74 0.69 17082
-分类报告净化后：
accuracy   0.95 5839
macro avg 0.94  0.92  0.93  5839
weighted avg 0.95  0.95  0.95  5839	
苏宁易购  2017年7月至2018年8月
自然语言处理工程师/短文本分类、商品属性词近义词抽取、纠错改写
•	工作内容
我负责了一个千万级商品分类项目，采用了C++多线程技术和朴素贝叶斯模型，旨在提高处理效率和分类准确性，以增强用户搜索体验。同时，我利用NLTK工具包进行商品标题近义词挖掘，丰富了搜索引擎关键词库，提高了搜索结果的准确性和用户体验。通过预处理、分词和构建词共现矩阵，实现了近义词抽取，捕捉了词语的语义关联和相似度。我的工作成果包括每周评测和持续监测商品分类模型的准确率和召回率，确保其维持在90%以上，提高了分类的准确性和稳定性。通过参数调优和算法改进，我不断优化了模型性能，及时响应用户反馈，提升了系统稳定性和可靠性。利用近义词挖掘技术丰富了搜索引擎关键词库，改进了系统功能和性能，提高了搜索结果准确性和用户体验。
•	商品标题分类项目
该项目是一个商品标题分类项目，利用了C++多线程技术和朴素贝叶斯模型。它包括了数据预处理模块、训练模块和预测模块，通过统计词到类目的映射来实现商品标题的分类。每周组织产品工程师进行模型效果评测，给出每周模型的准确率和召回率，并随时维护线上服务，快速进行人工干预以防止badcase的出现。
同方知网  2015年3月至2017年7月
自然语言处理工程师/论文文本查重、论文选题推荐、短文本用词纠错改写
•	工作内容
我在同方知网完成了论文抄袭检测任务，采用了C++编程语言。我设计了基于标点符号的特征抽取方法，通过哈希映射将特征存储到MySQL数据库中，实现了有效的论文抄袭检测。这些工作有助于维护学术界的诚信，有效杜绝了学术造假的现象，保障了科研的可信度和学术的真实性。
•	”的地得“语法纠错项目
我负责了一个名为“的地得”语法纠错项目。该项目采用了最大熵模型，旨在识别和纠正文本中“的地得”用法错误。首先，我们搜集了大量的文本数据，并进行了预处理，包括分词、词性标注等。然后，我们设计了特征抽取方法，将文本中的“的地得”用法错误作为目标，提取了与其相关的上下文信息、词性信息等特征。接下来，我们使用最大熵模型进行训练，以学习正确的“的地得”用法，并对文本中的错误进行识别和纠正。最后，我们对模型进行评估和调优，确保其在实际应用中具有较高的准确性和可靠性。通过这个项目，我们能够有效提高文本的语法准确性，提升用户阅读体验。
技能
Python、C++、CRF、MaxEnt、LSTM、TensorFlow、PyTorch、Transformers、Langchain框架。大学英语CET-6。
教育背景
2005 年 9 月-2009 年 7 月 北京信息科技大学 电子信息工程 统招本科
2012 年 9 月-2015 年 3 月 北京信息科技大学 电子与通信工程 统招硕士
